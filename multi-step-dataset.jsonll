{"prompt": "Create a comprehensive system monitoring script that checks CPU usage, memory usage, disk space, and network connectivity, then sends different alert levels (warning/critical) to different notification channels based on thresholds", "completion": "#!/bin/bash; CPU=$(top -bn1 | grep 'Cpu(s)' | awk '{print $2}' | cut -d'%' -f1); MEM=$(free | grep Mem | awk '{printf \"%.0f\", $3/$2 * 100.0}'); DISK=$(df / | tail -1 | awk '{print $5}' | cut -d'%' -f1); ping -c1 8.8.8.8 > /dev/null 2>&1; NET=$?; if [ $CPU -gt 90 ] || [ $MEM -gt 90 ] || [ $DISK -gt 90 ]; then echo \"CRITICAL: CPU:$CPU% MEM:$MEM% DISK:$DISK%\" | mail -s \"Critical Alert\" admin@company.com; elif [ $CPU -gt 70 ] || [ $MEM -gt 70 ] || [ $DISK -gt 70 ]; then echo \"WARNING: CPU:$CPU% MEM:$MEM% DISK:$DISK%\" | logger -p user.warn; fi; [ $NET -ne 0 ] && echo \"Network connectivity failed\" | mail -s \"Network Alert\" network@company.com"}
{"prompt": "Write a bash script that performs automated database backup with rotation, compression, encryption, and remote storage, including error handling and logging with different retention policies for daily, weekly, and monthly backups", "completion": "#!/bin/bash; LOG=\"/var/log/backup.log\"; DATE=$(date +%Y%m%d); DAILY_DIR=\"/backup/daily\"; WEEKLY_DIR=\"/backup/weekly\"; MONTHLY_DIR=\"/backup/monthly\"; mkdir -p $DAILY_DIR $WEEKLY_DIR $MONTHLY_DIR; echo \"$(date): Starting backup\" >> $LOG; if mysqldump -u root -p$DB_PASS --all-databases | gzip > $DAILY_DIR/db_$DATE.sql.gz 2>>$LOG; then gpg --cipher-algo AES256 --compress-algo 1 --symmetric --output $DAILY_DIR/db_$DATE.sql.gz.gpg $DAILY_DIR/db_$DATE.sql.gz && rm $DAILY_DIR/db_$DATE.sql.gz; rsync -avz $DAILY_DIR/db_$DATE.sql.gz.gpg user@backup-server:/backups/daily/ >> $LOG 2>&1; [ $(date +%w) -eq 0 ] && cp $DAILY_DIR/db_$DATE.sql.gz.gpg $WEEKLY_DIR/; [ $(date +%d) -eq 01 ] && cp $DAILY_DIR/db_$DATE.sql.gz.gpg $MONTHLY_DIR/; find $DAILY_DIR -name '*.gpg' -mtime +7 -delete; find $WEEKLY_DIR -name '*.gpg' -mtime +30 -delete; find $MONTHLY_DIR -name '*.gpg' -mtime +365 -delete; echo \"$(date): Backup completed successfully\" >> $LOG; else echo \"$(date): Backup failed\" >> $LOG; echo \"Database backup failed on $(hostname)\" | mail -s \"Backup Failure\" admin@company.com; exit 1; fi"}
{"prompt": "Create a sophisticated log analysis script that parses web server access logs, identifies security threats (SQL injection, XSS attempts, brute force attacks), generates statistical reports with top attackers, most targeted URLs, and implements automatic IP blocking with whitelist exceptions", "completion": "#!/bin/bash; LOG_FILE=\"/var/log/apache2/access.log\"; BLOCK_LIST=\"/tmp/blocked_ips.txt\"; WHITELIST=\"/etc/security/whitelist.txt\"; REPORT=\"/var/log/security_report_$(date +%Y%m%d).txt\"; echo \"Security Analysis Report - $(date)\" > $REPORT; echo \"===========================================\" >> $REPORT; awk '{print $1}' $LOG_FILE | grep -E '^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$' | sort | uniq -c | sort -nr | head -20 > /tmp/top_ips.txt; echo \"Top 20 IP Addresses:\" >> $REPORT; cat /tmp/top_ips.txt >> $REPORT; grep -i -E '(select.*from|union.*select|script.*alert|javascript:|<script|drop.*table|insert.*into|update.*set)' $LOG_FILE | awk '{print $1}' | sort | uniq > /tmp/malicious_ips.txt; echo -e \"\\nMalicious Activity Detected from IPs:\" >> $REPORT; cat /tmp/malicious_ips.txt >> $REPORT; awk '$9 == \"401\" || $9 == \"403\" {print $1}' $LOG_FILE | sort | uniq -c | awk '$1 > 10 {print $2}' > /tmp/brute_force_ips.txt; echo -e \"\\nBrute Force Attempts:\" >> $REPORT; cat /tmp/brute_force_ips.txt >> $REPORT; cat /tmp/malicious_ips.txt /tmp/brute_force_ips.txt | sort | uniq > $BLOCK_LIST; for ip in $(cat $BLOCK_LIST); do if ! grep -q $ip $WHITELIST 2>/dev/null; then iptables -C INPUT -s $ip -j DROP 2>/dev/null || iptables -I INPUT -s $ip -j DROP; echo \"Blocked IP: $ip\" >> $REPORT; fi; done; awk '{print $7}' $LOG_FILE | sort | uniq -c | sort -nr | head -10 >> $REPORT; mail -s \"Security Report $(date +%Y-%m-%d)\" security@company.com < $REPORT"}
{"prompt": "Develop a comprehensive system health checker that monitors system resources, validates service dependencies, performs application-specific health checks, generates detailed reports with graphs, and automatically triggers remediation actions based on predefined rules", "completion": "#!/bin/bash; HEALTH_DIR=\"/var/log/health\"; mkdir -p $HEALTH_DIR; REPORT=\"$HEALTH_DIR/health_$(date +%Y%m%d_%H%M%S).json\"; echo '{\"timestamp\":\"'$(date -Iseconds)'\",\"hostname\":\"'$(hostname)'\",\"checks\":{' > $REPORT; CPU_USAGE=$(top -bn1 | grep 'Cpu(s)' | awk '{print $2}' | cut -d'%' -f1 | cut -d'.' -f1); MEM_USAGE=$(free | grep Mem: | awk '{printf \"%.0f\", $3/$2 * 100.0}'); DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | cut -d'%' -f1); LOAD_AVG=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | sed 's/,//'); SERVICE_STATUS=''; for service in nginx mysql redis; do systemctl is-active $service > /dev/null 2>&1 && SERVICE_STATUS=\"$SERVICE_STATUS\\\"$service\\\":\\\"running\\\",\" || SERVICE_STATUS=\"$SERVICE_STATUS\\\"$service\\\":\\\"failed\\\",\"; done; HTTP_CHECK=$(curl -s -o /dev/null -w \"%{http_code}\" http://localhost 2>/dev/null); DB_CHECK=$(mysql -u monitor -p$MONITOR_PASS -e \"SELECT 1\" 2>/dev/null && echo \"ok\" || echo \"failed\"); echo '\"system\":{\"cpu\":'$CPU_USAGE',\"memory\":'$MEM_USAGE',\"disk\":'$DISK_USAGE',\"load\":\"'$LOAD_AVG'\"},\"services\":{'${SERVICE_STATUS%,}'},\"application\":{\"http_status\":'$HTTP_CHECK',\"database\":\"'$DB_CHECK'\"}}}' >> $REPORT; if [ $CPU_USAGE -gt 80 ] || [ $MEM_USAGE -gt 80 ]; then systemctl restart high-memory-service; echo \"$(date): Auto-restarted services due to high resource usage\" >> /var/log/auto-remediation.log; fi; [ $HTTP_CHECK -ne 200 ] && systemctl restart nginx && echo \"$(date): Restarted nginx due to HTTP check failure\" >> /var/log/auto-remediation.log; python3 -c \"import json,matplotlib.pyplot as plt; data=json.load(open('$REPORT')); plt.figure(figsize=(12,8)); plt.subplot(2,2,1); plt.bar(['CPU','Memory','Disk'],[data['checks']['system']['cpu'],data['checks']['system']['memory'],data['checks']['system']['disk']]); plt.title('Resource Usage %'); plt.savefig('$HEALTH_DIR/health_$(date +%Y%m%d_%H%M%S).png'); plt.close()\" 2>/dev/null || echo \"Graph generation failed\" >> /var/log/health.log"}
{"prompt": "Build a multi-threaded file synchronization script that compares directory structures across multiple remote servers, handles conflicts using timestamp and checksum validation, maintains sync logs with rollback capability, and supports bandwidth throttling and resume functionality", "completion": "#!/bin/bash; SYNC_LOG=\"/var/log/filesync.log\"; CONFLICT_LOG=\"/var/log/sync_conflicts.log\"; SERVERS=(\"server1.com\" \"server2.com\" \"server3.com\"); LOCAL_DIR=\"/data/shared\"; REMOTE_DIR=\"/data/shared\"; MAX_PARALLEL=4; BANDWIDTH=\"--bwlimit=1000\"; echo \"$(date): Starting multi-server sync\" >> $SYNC_LOG; declare -A file_hashes; declare -A file_times; sync_server() { local server=$1; echo \"$(date): Syncing with $server\" >> $SYNC_LOG; rsync -avz $BANDWIDTH --dry-run --itemize-changes $LOCAL_DIR/ $server:$REMOTE_DIR/ 2>/dev/null | while read line; do if [[ $line =~ ^[><].* ]]; then file=$(echo $line | awk '{print $2}'); local_hash=$(md5sum \"$LOCAL_DIR/$file\" 2>/dev/null | awk '{print $1}'); remote_hash=$(ssh $server \"md5sum '$REMOTE_DIR/$file'\" 2>/dev/null | awk '{print $1}'); local_time=$(stat -c %Y \"$LOCAL_DIR/$file\" 2>/dev/null || echo 0); remote_time=$(ssh $server \"stat -c %Y '$REMOTE_DIR/$file'\" 2>/dev/null || echo 0); if [ \"$local_hash\" != \"$remote_hash\" ]; then echo \"$(date): Conflict detected for $file on $server - Local: $local_hash ($local_time) Remote: $remote_hash ($remote_time)\" >> $CONFLICT_LOG; if [ $local_time -gt $remote_time ]; then rsync -avz $BANDWIDTH \"$LOCAL_DIR/$file\" $server:\"$REMOTE_DIR/$file\" >> $SYNC_LOG 2>&1; else rsync -avz $BANDWIDTH $server:\"$REMOTE_DIR/$file\" \"$LOCAL_DIR/$file\" >> $SYNC_LOG 2>&1; fi; fi; fi; done; }; export -f sync_server; export SYNC_LOG CONFLICT_LOG LOCAL_DIR REMOTE_DIR BANDWIDTH; echo \"${SERVERS[@]}\" | tr ' ' '\\n' | xargs -n1 -P$MAX_PARALLEL -I{} bash -c 'sync_server \"$@\"' _ {}; echo \"$(date): Sync completed\" >> $SYNC_LOG; if [ -s $CONFLICT_LOG ]; then echo \"Conflicts detected during sync:\" | mail -s \"Sync Conflicts - $(hostname)\" -a $CONFLICT_LOG admin@company.com; fi"}
{"prompt": "Create an advanced deployment script that manages blue-green deployments with health checks, database migrations with rollback support, zero-downtime switching, comprehensive logging, and integration with multiple environments including staging validation", "completion": "#!/bin/bash; APP_NAME=\"myapp\"; ENVIRONMENTS=(\"staging\" \"production\"); BLUE_PORT=8080; GREEN_PORT=8081; DB_MIGRATE_DIR=\"/app/migrations\"; HEALTH_ENDPOINT=\"/health\"; DEPLOY_LOG=\"/var/log/deploy_$(date +%Y%m%d_%H%M%S).log\"; ROLLBACK_DIR=\"/opt/rollbacks\"; deploy_environment() { local env=$1; local version=$2; echo \"$(date): Starting deployment to $env - version $version\" >> $DEPLOY_LOG; current_color=$(curl -s http://$env.company.com:9000/current-color 2>/dev/null || echo \"blue\"); target_color=$([ \"$current_color\" = \"blue\" ] && echo \"green\" || echo \"blue\"); target_port=$([ \"$target_color\" = \"blue\" ] && echo $BLUE_PORT || echo $GREEN_PORT); echo \"$(date): Deploying to $target_color environment on port $target_port\" >> $DEPLOY_LOG; mkdir -p $ROLLBACK_DIR/$env; cp -r /app $ROLLBACK_DIR/$env/app_$(date +%Y%m%d_%H%M%S); docker pull myregistry.com/$APP_NAME:$version >> $DEPLOY_LOG 2>&1; docker stop ${APP_NAME}_${target_color}_$env 2>/dev/null; docker run -d --name ${APP_NAME}_${target_color}_$env -p $target_port:8080 -e ENV=$env myregistry.com/$APP_NAME:$version >> $DEPLOY_LOG 2>&1; echo \"$(date): Waiting for application startup\" >> $DEPLOY_LOG; for i in {1..30}; do if curl -f http://localhost:$target_port$HEALTH_ENDPOINT > /dev/null 2>&1; then echo \"$(date): Health check passed\" >> $DEPLOY_LOG; break; fi; sleep 10; done; if ! curl -f http://localhost:$target_port$HEALTH_ENDPOINT > /dev/null 2>&1; then echo \"$(date): Health check failed, rolling back\" >> $DEPLOY_LOG; docker stop ${APP_NAME}_${target_color}_$env; return 1; fi; if [ \"$env\" = \"production\" ]; then mysql -u deploy -p$DB_PASS -e \"CREATE TABLE IF NOT EXISTS migrations (id INT AUTO_INCREMENT PRIMARY KEY, version VARCHAR(255), applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)\"; for migration in $DB_MIGRATE_DIR/*.sql; do version=$(basename $migration .sql); if ! mysql -u deploy -p$DB_PASS -e \"SELECT 1 FROM migrations WHERE version='$version'\" 2>/dev/null | grep -q 1; then echo \"$(date): Applying migration $version\" >> $DEPLOY_LOG; if mysql -u deploy -p$DB_PASS < $migration 2>>$DEPLOY_LOG; then mysql -u deploy -p$DB_PASS -e \"INSERT INTO migrations (version) VALUES ('$version')\"; else echo \"$(date): Migration $version failed, rolling back\" >> $DEPLOY_LOG; return 1; fi; fi; done; fi; nginx -s reload; sleep 5; old_port=$([ \"$current_color\" = \"blue\" ] && echo $BLUE_PORT || echo $GREEN_PORT); curl -X POST http://$env.company.com:9000/switch-color -d \"color=$target_color\" >> $DEPLOY_LOG 2>&1; sleep 10; docker stop ${APP_NAME}_${current_color}_$env 2>/dev/null; echo \"$(date): Deployment to $env completed successfully\" >> $DEPLOY_LOG; }; for env in \"${ENVIRONMENTS[@]}\"; do if deploy_environment $env $1; then echo \"$(date): $env deployment successful\" >> $DEPLOY_LOG; else echo \"$(date): $env deployment failed\" >> $DEPLOY_LOG; echo \"Deployment failed for $env\" | mail -s \"Deployment Failure\" -a $DEPLOY_LOG deploy@company.com; exit 1; fi; done"}
{"prompt": "Implement a comprehensive container orchestration script that manages Docker containers across multiple hosts, handles service discovery, load balancing configuration, health monitoring with auto-scaling, and maintains container logs with centralized aggregation", "completion": "#!/bin/bash; HOSTS=(\"docker-host1\" \"docker-host2\" \"docker-host3\"); SERVICE_NAME=\"web-service\"; MIN_INSTANCES=2; MAX_INSTANCES=10; CPU_THRESHOLD=70; MEMORY_THRESHOLD=80; CONSUL_HOST=\"consul.company.com\"; LOGSTASH_HOST=\"logstash.company.com\"; ORCHESTRATION_LOG=\"/var/log/orchestration.log\"; get_container_stats() { local host=$1; ssh $host \"docker stats --no-stream --format 'table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemPerc}}\\t{{.Name}}'\" | grep $SERVICE_NAME | while read line; do container=$(echo $line | awk '{print $1}'); cpu=$(echo $line | awk '{print $2}' | sed 's/%//'); memory=$(echo $line | awk '{print $3}' | sed 's/%//'); echo \"$host:$container:$cpu:$memory\"; done; }; scale_service() { local action=$1; local host=$2; echo \"$(date): Scaling $action on $host\" >> $ORCHESTRATION_LOG; if [ \"$action\" = \"up\" ]; then container_id=$(ssh $host \"docker run -d --name ${SERVICE_NAME}_$(date +%s) -p 0:8080 --log-driver=syslog --log-opt syslog-address=tcp://$LOGSTASH_HOST:514 --log-opt tag=$SERVICE_NAME myregistry.com/$SERVICE_NAME:latest\"); port=$(ssh $host \"docker port $container_id 8080\" | cut -d':' -f2); curl -X PUT http://$CONSUL_HOST:8500/v1/agent/service/register -d \"{\\\"ID\\\":\\\"$SERVICE_NAME-$host-$container_id\\\",\\\"Name\\\":\\\"$SERVICE_NAME\\\",\\\"Address\\\":\\\"$host\\\",\\\"Port\\\":$port,\\\"Check\\\":{\\\"HTTP\\\":\\\"http://$host:$port/health\\\",\\\"Interval\\\":\\\"10s\\\"}}\" >> $ORCHESTRATION_LOG 2>&1; else oldest_container=$(ssh $host \"docker ps --filter name=$SERVICE_NAME --format '{{.Names}} {{.CreatedAt}}' | sort -k2 | head -1 | awk '{print \\$1}'\"); if [ -n \"$oldest_container\" ]; then ssh $host \"docker stop $oldest_container && docker rm $oldest_container\"; curl -X PUT http://$CONSUL_HOST:8500/v1/agent/service/deregister/$SERVICE_NAME-$host-$oldest_container >> $ORCHESTRATION_LOG 2>&1; fi; fi; }; monitor_and_scale() { echo \"$(date): Starting monitoring cycle\" >> $ORCHESTRATION_LOG; total_instances=0; high_cpu_count=0; high_memory_count=0; for host in \"${HOSTS[@]}\"; do instance_count=$(ssh $host \"docker ps --filter name=$SERVICE_NAME -q\" | wc -l); total_instances=$((total_instances + instance_count)); get_container_stats $host | while IFS=: read host_name container_id cpu memory; do if (( $(echo \"$cpu > $CPU_THRESHOLD\" | bc -l) )) || (( $(echo \"$memory > $MEMORY_THRESHOLD\" | bc -l) )); then echo \"$(date): High resource usage detected on $host_name:$container_id (CPU: $cpu%, Memory: $memory%)\" >> $ORCHESTRATION_LOG; high_cpu_count=$((high_cpu_count + 1)); fi; done; done; if [ $high_cpu_count -gt 0 ] && [ $total_instances -lt $MAX_INSTANCES ]; then target_host=$(printf '%s\\n' \"${HOSTS[@]}\" | shuf | head -1); scale_service \"up\" $target_host; elif [ $high_cpu_count -eq 0 ] && [ $total_instances -gt $MIN_INSTANCES ]; then target_host=$(printf '%s\\n' \"${HOSTS[@]}\" | shuf | head -1); scale_service \"down\" $target_host; fi; for host in \"${HOSTS[@]}\"; do ssh $host \"docker ps --filter name=$SERVICE_NAME --format '{{.Names}}' | xargs -I {} docker exec {} /bin/sh -c 'echo \\$(date): Container {} on $host reporting status | nc $LOGSTASH_HOST 5000'\"; done; }; while true; do monitor_and_scale; sleep 60; done"}
{"prompt": "Develop a sophisticated network security scanner that performs port scanning, vulnerability assessment, SSL certificate validation, web application security testing, generates comprehensive reports with risk ratings, and integrates with threat intelligence feeds", "completion": "#!/bin/bash; TARGET_RANGE=\"$1\"; SCAN_TYPE=\"${2:-comprehensive}\"; OUTPUT_DIR=\"/var/log/security_scans/$(date +%Y%m%d_%H%M%S)\"; THREAT_INTEL_API=\"https://api.threatintel.com/v1/lookup\"; VULN_DB=\"/opt/security/vulndb.txt\"; SSL_DAYS_WARNING=30; mkdir -p $OUTPUT_DIR; echo \"$(date): Starting security scan of $TARGET_RANGE\" > $OUTPUT_DIR/scan.log; nmap -sS -O -sV -sC -p 1-65535 --script vuln,ssl-cert,http-enum,smb-vuln* $TARGET_RANGE -oX $OUTPUT_DIR/nmap_results.xml -oN $OUTPUT_DIR/nmap_results.txt >> $OUTPUT_DIR/scan.log 2>&1; xmllint --xpath \"//host[status[@state='up']]\" $OUTPUT_DIR/nmap_results.xml 2>/dev/null | grep -oP 'addr=\"\\K[^\"]*' > $OUTPUT_DIR/live_hosts.txt; echo \"{\\\"scan_id\\\":\\\"$(uuidgen)\\\",\\\"timestamp\\\":\\\"$(date -Iseconds)\\\",\\\"target\\\":\\\"$TARGET_RANGE\\\",\\\"findings\\\":[\" > $OUTPUT_DIR/security_report.json; risk_score=0; while read host; do echo \"$(date): Scanning $host for vulnerabilities\" >> $OUTPUT_DIR/scan.log; nmap --script ssl-cert $host -p 443 2>/dev/null | grep -A5 \"Not valid after\" | tail -1 | awk '{print $4\" \"$5\" \"$6}' | while read ssl_date; do if [ -n \"$ssl_date\" ]; then ssl_expiry=$(date -d \"$ssl_date\" +%s 2>/dev/null || echo 0); current_time=$(date +%s); days_left=$(( (ssl_expiry - current_time) / 86400 )); if [ $days_left -lt $SSL_DAYS_WARNING ]; then echo \"{\\\"host\\\":\\\"$host\\\",\\\"vulnerability\\\":\\\"SSL Certificate Expiring\\\",\\\"severity\\\":\\\"medium\\\",\\\"days_until_expiry\\\":$days_left},\" >> $OUTPUT_DIR/security_report.json; risk_score=$((risk_score + 5)); fi; fi; done; open_ports=$(nmap -p- --open $host 2>/dev/null | grep \"^[0-9]\" | cut -d'/' -f1 | tr '\\n' ',' | sed 's/,$//'); for port in $(echo $open_ports | tr ',' ' '); do service=$(nmap -sV $host -p $port 2>/dev/null | grep \"$port/tcp\" | awk '{print $3\" \"$4\" \"$5}'); if echo \"$service\" | grep -qiE '(ssh|telnet|ftp|smtp|pop3|imap|http|https|snmp|ldap|mysql|postgresql|mssql|oracle|redis|mongodb)'; then echo \"{\\\"host\\\":\\\"$host\\\",\\\"port\\\":$port,\\\"service\\\":\\\"$service\\\",\\\"risk\\\":\\\"info\\\"},\" >> $OUTPUT_DIR/security_report.json; fi; if [ \"$port\" = \"23\" ] || [ \"$port\" = \"21\" ] || [ \"$port\" = \"135\" ]; then echo \"{\\\"host\\\":\\\"$host\\\",\\\"port\\\":$port,\\\"vulnerability\\\":\\\"Insecure Service\\\",\\\"severity\\\":\\\"high\\\",\\\"service\\\":\\\"$service\\\"},\" >> $OUTPUT_DIR/security_report.json; risk_score=$((risk_score + 10)); fi; done; if curl -s --connect-timeout 5 http://$host 2>/dev/null | grep -qi \"server:\"; then server_header=$(curl -sI http://$host 2>/dev/null | grep -i \"server:\" | cut -d' ' -f2-); echo \"{\\\"host\\\":\\\"$host\\\",\\\"finding\\\":\\\"Server Header Disclosure\\\",\\\"severity\\\":\\\"low\\\",\\\"details\\\":\\\"$server_header\\\"},\" >> $OUTPUT_DIR/security_report.json; risk_score=$((risk_score + 2)); fi; curl -s \"$THREAT_INTEL_API?ip=$host&key=$THREAT_API_KEY\" 2>/dev/null | jq -r '.reputation' 2>/dev/null | grep -q \"malicious\" && echo \"{\\\"host\\\":\\\"$host\\\",\\\"vulnerability\\\":\\\"Malicious IP\\\",\\\"severity\\\":\\\"critical\\\",\\\"source\\\":\\\"threat_intelligence\\\"},\" >> $OUTPUT_DIR/security_report.json && risk_score=$((risk_score + 20)); done < $OUTPUT_DIR/live_hosts.txt; echo \"],\\\"risk_score\\\":$risk_score,\\\"risk_level\\\":\\\"$([ $risk_score -gt 50 ] && echo 'high' || [ $risk_score -gt 20 ] && echo 'medium' || echo 'low')\\\"}}\" >> $OUTPUT_DIR/security_report.json; python3 -c \"import json,sys; data=json.load(open('$OUTPUT_DIR/security_report.json')); print('Security Scan Report'); print('=================='); print(f'Target: {data[\\\"target\\\"]}'); print(f'Risk Score: {data[\\\"risk_score\\\"]} ({data[\\\"risk_level\\\"].upper()})'); print(f'Findings: {len(data[\\\"findings\\\"])}'); [print(f\\\"- {f.get('host','N/A')}: {f.get('vulnerability',f.get('finding','Unknown'))} ({f.get('severity','unknown')})\\\") for f in data['findings']]\" > $OUTPUT_DIR/summary_report.txt; [ $risk_score -gt 30 ] && mail -s \"High Risk Security Scan Results\" -a $OUTPUT_DIR/summary_report.txt security@company.com < $OUTPUT_DIR/summary_report.txt"}
{"prompt": "Create an intelligent data pipeline script that ingests data from multiple sources (APIs, databases, files), performs ETL transformations with data validation and error handling, implements data quality checks, and loads into target systems with monitoring and alerting", "completion": "#!/bin/bash; PIPELINE_CONFIG=\"/etc/datapipeline/config.json\"; LOG_DIR=\"/var/log/datapipeline\"; ERROR_THRESHOLD=5; QUALITY_THRESHOLD=95; MONITORING_ENDPOINT=\"http://monitoring.company.com/api/metrics\"; mkdir -p $LOG_DIR; PIPELINE_ID=$(uuidgen); RUN_LOG=\"$LOG_DIR/pipeline_${PIPELINE_ID}_$(date +%Y%m%d_%H%M%S).log\"; echo \"$(date): Starting data pipeline $PIPELINE_ID\" > $RUN_LOG; extract_data() { local source_type=$1; local source_config=$2; local output_file=$3; echo \"$(date): Extracting from $source_type\" >> $RUN_LOG; case $source_type in \"api\") endpoint=$(echo $source_config | jq -r '.endpoint'); auth_header=$(echo $source_config | jq -r '.auth_header'); retry_count=0; while [ $retry_count -lt 3 ]; do if curl -H \"$auth_header\" -f \"$endpoint\" -o \"$output_file\" 2>>$RUN_LOG; then echo \"$(date): API extraction successful\" >> $RUN_LOG; return 0; fi; retry_count=$((retry_count + 1)); sleep $((retry_count * 5)); done; echo \"$(date): API extraction failed after retries\" >> $RUN_LOG; return 1 ;; \"database\") db_host=$(echo $source_config | jq -r '.host'); db_name=$(echo $source_config | jq -r '.database'); query=$(echo $source_config | jq -r '.query'); mysql -h $db_host -u $DB_USER -p$DB_PASS $db_name -e \"$query\" > \"$output_file\" 2>>$RUN_LOG; [ $? -eq 0 ] && echo \"$(date): Database extraction successful\" >> $RUN_LOG || { echo \"$(date): Database extraction failed\" >> $RUN_LOG; return 1; } ;; \"file\") file_path=$(echo $source_config | jq -r '.path'); if [ -f \"$file_path\" ]; then cp \"$file_path\" \"$output_file\"; echo \"$(date): File extraction successful\" >> $RUN_LOG; else echo \"$(date): Source file not found: $file_path\" >> $RUN_LOG; return 1; fi ;; esac; }; transform_data() { local input_file=$1; local output_file=$2; local transformations=$3; echo \"$(date): Starting data transformations\" >> $RUN_LOG; cp \"$input_file\" \"/tmp/transform_temp.csv\"; echo $transformations | jq -c '.[]' | while read transform; do transform_type=$(echo $transform | jq -r '.type'); case $transform_type in \"remove_duplicates\") key_column=$(echo $transform | jq -r '.key_column'); awk -F',' '!seen[$'$key_column']++' \"/tmp/transform_temp.csv\" > \"/tmp/transform_temp2.csv\" && mv \"/tmp/transform_temp2.csv\" \"/tmp/transform_temp.csv\" ;; \"filter_rows\") condition=$(echo $transform | jq -r '.condition'); awk -F',' '$0 ~ /'$condition'/' \"/tmp/transform_temp.csv\" > \"/tmp/transform_temp2.csv\" && mv \"/tmp/transform_temp2.csv\" \"/tmp/transform_temp.csv\" ;; \"add_column\") column_name=$(echo $transform | jq -r '.name'); column_value=$(echo $transform | jq -r '.value'); awk -F',' -v col=\"$column_value\" 'BEGIN{OFS=\",\"} NR==1{print $0,\"'$column_name'\"} NR>1{print $0,col}' \"/tmp/transform_temp.csv\" > \"/tmp/transform_temp2.csv\" && mv \"/tmp/transform_temp2.csv\" \"/tmp/transform_temp.csv\" ;; \"data_type_conversion\") column=$(echo $transform | jq -r '.column'); target_type=$(echo $transform | jq -r '.target_type'); python3 -c \"import pandas as pd; df=pd.read_csv('/tmp/transform
